'''
ICEQ (2025) - –ú–æ–¥—É–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤

–û—Å–Ω–æ–≤–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª:
- –ö–ª–∞—Å—Å QuestionsGenerator –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É
- –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
    >>> generator = QuestionsGenerator()
    >>> questions = generator.generate(text, 10)
'''

from typing import Literal, List, Dict

import re
import os
import json

import torch
import faiss
import numpy as np
from dotenv import load_dotenv
from openai import OpenAI
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from question_generator_api import generate_questions_deepseek, generate_questions_qwen
import asyncio

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∫–æ–¥–∏—Ä–æ–≤–æ–∫
try:
    # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –ø—É—Ç—å –∫ .env –≤ –∫–æ—Ä–Ω–µ–≤–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    current_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.dirname(current_dir)
    env_path = os.path.join(project_root, '.env')

    if os.path.exists(env_path):
        loaded_successfully = False
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–¥–∏—Ä–æ–≤–∫–∏, —á—Ç–æ–±—ã —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è —Å BOM –≤ Windows
        for encoding in ['utf-8-sig', 'utf-16', 'utf-8', 'cp1251']:
            try:
                # override=True —á—Ç–æ–±—ã –Ω–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞–º–µ–Ω–∏–ª–∏ —Å—Ç–∞—Ä—ã–µ, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
                if load_dotenv(dotenv_path=env_path, encoding=encoding, override=True):
                    loaded_successfully = True
                    break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
            except Exception:
                continue # –ü—Ä–æ–±—É–µ–º —Å–ª–µ–¥—É—é—â—É—é –∫–æ–¥–∏—Ä–æ–≤–∫—É

        if not loaded_successfully:
            print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å .env —Ñ–∞–π–ª. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —Å–æ—Ö—Ä–∞–Ω–∏—Ç–µ –µ–≥–æ –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ UTF-8.")
    else:
        # Fallback, –µ—Å–ª–∏ .env –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –∫–æ—Ä–Ω–µ
        if load_dotenv(override=True):
            print("‚ÑπÔ∏è .env —Ñ–∞–π–ª –≤ –∫–æ—Ä–Ω–µ –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–æ–∏—Å–∫.")
        else:
            print("‚ö†Ô∏è .env —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –Ω–∏ –≤ –æ–¥–Ω–æ–º –∏–∑ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã—Ö –º–µ—Å—Ç.")

except Exception as e:
    print(f"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ .env: {e}")

# –ß–∞—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
DATA_PART = 0.01
# –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
MIN_CLUSTERS_NUM = 10
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
MAX_CLUSTERS_NUM = 500

# –¢–µ–≥ –≤ —Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–º–ø—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤
QUESTIONS_NUM_PROMPT_TAG = '[QUESTIONS_NUM]'
# –¢–µ–≥ –≤ —Ç–µ–∫—Å—Ç–µ –ø—Ä–æ–º–ø—Ç–∞, –∫–æ—Ç–æ—Ä—ã–π –Ω—É–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
CHUNKS_PROMPT_TAG = '[CHUNKS]'

ICEQ_MODEL_NAME = 'iceq_model'


def parse_questions(text_questions: str) -> list[dict]:
    """
    –ü–∞—Ä—Å–∏—Ç JSON —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
    
    Args:
        text_questions (str): JSON —Ç–µ–∫—Å—Ç —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏
    
    Returns:
        list[dict]: –°–ø–∏—Å–æ–∫ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
    """
    print("üîç === –ù–ê–ß–ê–õ–û –ü–ê–†–°–ò–ù–ì–ê –í–û–ü–†–û–°–û–í ===")
    print(f"üìù –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {len(text_questions)} —Å–∏–º–≤–æ–ª–æ–≤")
    print(f"üìÑ –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞: {text_questions[:300]}...")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ markdown –±–ª–æ–∫–æ–≤ –µ—Å–ª–∏ –µ—Å—Ç—å
    json_text = text_questions.strip()
    if json_text.startswith('```json'):
        print("üîç –û–±–Ω–∞—Ä—É–∂–µ–Ω JSON –≤ markdown –±–ª–æ–∫–µ, –∏–∑–≤–ª–µ–∫–∞–µ–º...")
        # –ò—â–µ–º –Ω–∞—á–∞–ª–æ –∏ –∫–æ–Ω–µ—Ü JSON –±–ª–æ–∫–∞
        start_marker = '```json'
        end_marker = '```'
        
        start_idx = json_text.find(start_marker) + len(start_marker)
        end_idx = json_text.find(end_marker, start_idx)
        
        if end_idx != -1:
            json_text = json_text[start_idx:end_idx].strip()
            print(f"‚úÖ JSON –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ markdown, –¥–ª–∏–Ω–∞: {len(json_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π JSON: {json_text[:200]}...")
        else:
            print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π –º–∞—Ä–∫–µ—Ä –¥–ª—è JSON –±–ª–æ–∫–∞")
    elif '```json' in json_text:
        print("üîç JSON –±–ª–æ–∫ –Ω–∞–π–¥–µ–Ω –Ω–µ –≤ –Ω–∞—á–∞–ª–µ, –∏–∑–≤–ª–µ–∫–∞–µ–º...")
        # –ò—â–µ–º JSON –±–ª–æ–∫ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞
        start_marker = '```json'
        end_marker = '```'
        
        start_idx = json_text.find(start_marker)
        if start_idx != -1:
            start_idx += len(start_marker)
            end_idx = json_text.find(end_marker, start_idx)
            
            if end_idx != -1:
                json_text = json_text[start_idx:end_idx].strip()
                print(f"‚úÖ JSON –∏–∑–≤–ª–µ—á–µ–Ω –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞, –¥–ª–∏–Ω–∞: {len(json_text)} —Å–∏–º–≤–æ–ª–æ–≤")
                print(f"üìÑ –ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–π JSON: {json_text[:200]}...")
            else:
                print("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π –º–∞—Ä–∫–µ—Ä –¥–ª—è JSON –±–ª–æ–∫–∞")
    else:
        print("üîç Markdown –±–ª–æ–∫–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –µ—Å—Ç—å")
    
    try:
        # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –∫–∞–∫ JSON
        print("üîç –ü–æ–ø—ã—Ç–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–∫ JSON...")
        json_data = json.loads(json_text)
        print("‚úÖ JSON —É—Å–ø–µ—à–Ω–æ —Ä–∞—Å–ø–∞—Ä—Å–µ–Ω!")
        
        questions = []
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã JSON
        if isinstance(json_data, dict) and 'questions' in json_data:
            print(f"üìä –ù–∞–π–¥–µ–Ω –æ–±—ä–µ–∫—Ç —Å –ø–æ–ª–µ–º 'questions', —Å–æ–¥–µ—Ä–∂–∏—Ç {len(json_data['questions'])} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            items = json_data['questions']
        elif isinstance(json_data, list):
            print(f"üìä –ù–∞–π–¥–µ–Ω —Å–ø–∏—Å–æ–∫ –∏–∑ {len(json_data)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            items = json_data
        else:
            print(f"‚ö†Ô∏è –ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç JSON: {type(json_data)}")
            items = []
        
        for i, item in enumerate(items):
            print(f"üîç –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ {i+1}: {item}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–æ–ª–µ–π
            if not isinstance(item, dict):
                print(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç {i+1} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –æ–±—ä–µ–∫—Ç–æ–º: {type(item)}")
                continue
                
            question_text = item.get('question', '')
            if not question_text:
                print(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç {i+1} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–æ–ø—Ä–æ—Å–∞")
                continue
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
            options = item.get('options', item.get('answers', []))
            if not options:
                print(f"‚ö†Ô∏è –≠–ª–µ–º–µ–Ω—Ç {i+1} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ –æ—Ç–≤–µ—Ç–æ–≤")
                continue
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            correct_answer = item.get('correct_answer', '')
            
            print(f"‚ùì –í–æ–ø—Ä–æ—Å: {question_text}")
            print(f"üìã –í–∞—Ä–∏–∞–Ω—Ç—ã: {options}")
            print(f"‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: {correct_answer}")
            
            # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –æ—Ç–≤–µ—Ç–æ–≤ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
            answers = []
            for j, option in enumerate(options):
                # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å —Å—Ç—Ä–æ–∫–æ–π (—Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞) –∏–ª–∏ –Ω–æ–º–µ—Ä–æ–º
                is_correct = False
                
                if isinstance(correct_answer, str):
                    # –ï—Å–ª–∏ correct_answer - —Å—Ç—Ä–æ–∫–∞, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å —Ç–µ–∫—Å—Ç–æ–º –≤–∞—Ä–∏–∞–Ω—Ç–∞
                    is_correct = (option.strip() == correct_answer.strip())
                elif isinstance(correct_answer, int):
                    # –ï—Å–ª–∏ correct_answer - –Ω–æ–º–µ—Ä (1-based)
                    is_correct = (j + 1 == correct_answer)
                
                answers.append({
                    'answer': option.strip(),
                    'is_correct': is_correct
                })
                
                print(f"  {j+1}. {option} {'‚úÖ' if is_correct else '‚ùå'}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            correct_count = sum(1 for ans in answers if ans['is_correct'])
            if correct_count == 0:
                print(f"‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {i+1}, –ø—ã—Ç–∞–µ–º—Å—è –∏—Å–ø—Ä–∞–≤–∏—Ç—å...")
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Ç–æ—á–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è, –ø—Ä–æ–±—É–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ
                for j, option in enumerate(options):
                    if correct_answer.lower() in option.lower() or option.lower() in correct_answer.lower():
                        answers[j]['is_correct'] = True
                        print(f"üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é: {option}")
                        break
                else:
                    # –ï—Å–ª–∏ –≤—Å–µ –µ—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω, –¥–µ–ª–∞–µ–º –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º
                    if answers:
                        answers[0]['is_correct'] = True
                        print(f"üîß –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–µ—Ä–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –∫–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
            
            questions.append({
                'question': question_text,
                'answers': answers,
                'explanation': item.get('explanation', '')
            })
            print(f"‚úÖ –í–æ–ø—Ä–æ—Å {i+1} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω")
        
        print(f"‚úÖ JSON –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–æ–ª—É—á–µ–Ω–æ {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
        if questions:
            print("üîç === –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê ===")
            for i, q in enumerate(questions):
                print(f"  {i+1}. {q['question'][:50]}... ({len(q['answers'])} –æ—Ç–≤–µ—Ç–æ–≤)")
            print("=== –ö–û–ù–ï–¶ –ü–ê–†–°–ò–ù–ì–ê –í–û–ü–†–û–°–û–í ===")
            return questions
            
    except json.JSONDecodeError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON: {e}")
        print("üîç –ü–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø–∞—Ä—Å–∏–Ω–≥—É –≤ —Å—Ç–∞—Ä–æ–º —Ñ–æ—Ä–º–∞—Ç–µ...")
    
    # –ï—Å–ª–∏ JSON –Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è, –ø—Ä–æ–±—É–µ–º —Å—Ç–∞—Ä—ã–π —Ñ–æ—Ä–º–∞—Ç
    print("üîç –ü—Ä–∏–º–µ–Ω—è–µ–º —Ä–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∞...")
    question_block_pattern = re.compile(r'(?ms)^\s*(\d+)\.\s*(.+?)(?=^\s*\d+\.\s|\Z)')
    answer_line_pattern = re.compile(r'^\s*([+-])\s*(.+)')
    explanation_line_pattern = re.compile(r'^\s*!\s*(.+)')

    questions = []
    blocks = question_block_pattern.findall(text_questions)
    print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(blocks)} –±–ª–æ–∫–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤")
    
    for i, (number, block) in enumerate(blocks):
        lines = block.splitlines()
        if not lines:
            continue
        
        question_text = lines[0].strip()
        answers = []
        explanation = ""

        for j, line in enumerate(lines[1:], 1):
            line = line.strip()
            if not line:
                continue
            
            answer_match = answer_line_pattern.match(line)
            explanation_match = explanation_line_pattern.match(line)

            if answer_match:
                sign, answer_text = answer_match.groups()
                is_correct = (sign == '+')
                answers.append({
                    'answer': answer_text.strip(),
                    'is_correct': is_correct
                })
            elif explanation_match:
                explanation = explanation_match.group(1).strip()

        if answers:
            questions.append({
                'question': question_text,
                'answers': answers,
                'explanation': explanation
            })
        else:
            print(f"‚ö†Ô∏è –ë–ª–æ–∫ {i+1} –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
    
    print(f"üîç === –†–ï–ó–£–õ–¨–¢–ê–¢ –ü–ê–†–°–ò–ù–ì–ê ===")
    print(f"üìä –ò—Ç–æ–≥–æ –Ω–∞–π–¥–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(questions)}")
    for i, q in enumerate(questions):
        print(f"  {i+1}. {q['question'][:50]}... ({len(q['answers'])} –æ—Ç–≤–µ—Ç–æ–≤)")
    
    if not questions:
        print("‚ùå –í–ù–ò–ú–ê–ù–ò–ï: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞!")
        print("üìÑ –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –ª—é–±—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —Ç–µ–∫—Å—Ç–µ:")
        # –ò—â–µ–º —á–∏—Å–ª–∞ –≤ –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫
        numbered_lines = re.findall(r'^\d+\..*', text_questions, re.MULTILINE)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å –Ω–æ–º–µ—Ä–∞–º–∏: {len(numbered_lines)}")
        for line in numbered_lines[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {line}")
        
        # –ò—â–µ–º –∑–Ω–∞–∫–∏ + –∏ -
        plus_minus_lines = re.findall(r'^[+-].*', text_questions, re.MULTILINE)
        print(f"üîç –ù–∞–π–¥–µ–Ω–æ —Å—Ç—Ä–æ–∫ —Å +/-: {len(plus_minus_lines)}")
        for line in plus_minus_lines[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {line}")
    
    print("=== –ö–û–ù–ï–¶ –ü–ê–†–°–ò–ù–ì–ê –í–û–ü–†–û–°–û–í ===")
    return questions


class QuestionsGenerator:
    """
    –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
    
    –ö–ª–∞—Å—Å —Ä–µ–∞–ª–∏–∑—É–µ—Ç –ø–∞—Ç—Ç–µ—Ä–Ω Singleton –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è:
    - –ê–Ω–∞–ª–∏–∑–∞ –∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    - –ì–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é DeepSeek API –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ ICEQ
    - –î–æ–±–∞–≤–ª–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
    - –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    
    Attributes:
        device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cuda' –∏–ª–∏ 'cpu')
        deepseek_client (OpenAI): –ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DeepSeek API
        iceq_model (dict): –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å ICEQ —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º
    
    Examples:
        >>> # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ —Å DeepSeek
        >>> generator = QuestionsGenerator(init_llms=['deepseek'])
        >>> 
        >>> # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 5 –≤–æ–ø—Ä–æ—Å–æ–≤
        >>> with open('text.txt', 'r', encoding='utf8') as f:
        >>>     text = f.read()
        >>> questions = generator.generate(text, 5, llm='deepseek')
        >>> 
        >>> # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        >>> for i, q in enumerate(questions, 1):
        >>>     print(f"–í–æ–ø—Ä–æ—Å {i}: {q['question']}")
        >>>     for ans in q['answers']:
        >>>         mark = '‚úì' if ans['is_correct'] else '‚úó'
        >>>         print(f"  {mark} {ans['answer']}")
    """

    _instance = None
    _initialized = False

    def __new__(cls, *args, **kwargs):
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–∞ Singleton"""
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self, init_llms: list = []):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –≤–æ–ø—Ä–æ—Å–æ–≤
        
        Args:
            init_llms (list): –°–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏.
                –î–æ—Å—Ç—É–ø–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: ['deepseek', 'iceq']
                –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è –ª–µ–Ω–∏–≤–æ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏
        """
        # –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ–º –ø–æ–≤—Ç–æ—Ä–Ω—É—é –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é Singleton
        if QuestionsGenerator._initialized:
            return None
        QuestionsGenerator._initialized = True

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–≥–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        if torch.cuda.is_available():
            self.device = 'cuda'
            torch.cuda.empty_cache()
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—É—é –ø–∞–º—è—Ç—å
            if hasattr(torch.cuda, 'reset_peak_memory_stats'):
                torch.cuda.reset_peak_memory_stats()
        else:
            print('–í–ù–ò–ú–ê–ù–ò–ï: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU')
            self.device = 'cpu'
        print(f'–ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}')

        # –õ–µ–Ω–∏–≤–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π
        self.deepseek_available = self.__init_deepseek() if 'deepseek' in init_llms else False
        # –ö–ª–∏–µ–Ω—Ç DeepSeek –±—É–¥–µ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –æ–±—Ä–∞—â–µ–Ω–∏–∏
        self.deepseek_client = None
        self.iceq_model = self.__init_iceq() if 'iceq' in init_llms else None

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        print('–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞...')
        self.__clustering_model = SentenceTransformer(
            'intfloat/multilingual-e5-large-instruct',  # –ú–æ–¥–µ–ª—å –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
            device=self.device
        )
        self.__search_model = SentenceTransformer(
            'ai-forever/FRIDA',  # –ú–æ–¥–µ–ª—å –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
            device=self.device
        )
        print('–ú–æ–¥–µ–ª–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Ç–µ–∫—Å—Ç–∞ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.')

        # –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤ –∏–∑ —Ñ–∞–π–ª–æ–≤
        print('–ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤...')
        import os
        script_dir = os.path.dirname(os.path.abspath(__file__))
        self.__user_prompt_template = self.__load_prompt(os.path.join(script_dir, 'user_prompt.txt'))
        self.__system_prompt = self.__load_prompt(os.path.join(script_dir, 'system_prompt.txt'))
        print('–ü—Ä–æ–º–ø—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã.')
        print('–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.')

    def __init_deepseek(self) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ API –∫–ª—é—á–∞ –¥–ª—è DeepSeek
        
        Returns:
            bool: True –µ—Å–ª–∏ –∫–ª—é—á –Ω–∞–π–¥–µ–Ω, False –µ—Å–ª–∏ –Ω–µ—Ç
        """
        api_key = os.getenv('DEEPSEEK_API_KEY')
        if not api_key:
            print("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è DeepSeek")
            return False
        
        print('DeepSeek API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω.')
        return True

    def __init_iceq(self) -> None:
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å ICEQ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ø–∞–º—è—Ç–∏
        
        –ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏:
        1. 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏)
        2. 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (fallback)
        3. CPU –∑–∞–≥—Ä—É–∑–∫–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–π fallback)
        
        Returns:
            dict: —Å–ª–æ–≤–∞—Ä—å —Å —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º –∏ –º–æ–¥–µ–ª—å—é –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            print('–ó–∞–≥—Ä—É–∑–∫–∞ ICEQ —Å –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û–ô –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π (4-bit)...')
            tokenizer = AutoTokenizer.from_pretrained('t-tech/T-lite-it-1.0')
            
            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏
            if self.device == 'cuda':
                try:
                    print('–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ ICEQ –Ω–∞ GPU —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...')
                    model = AutoModelForCausalLM.from_pretrained(
                        'droyti/ICEQ', 
                        torch_dtype=torch.float16,
                        device_map='auto',
                        low_cpu_mem_usage=True,
                        load_in_8bit=True,  # –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è –≤ 8 –±–∏—Ç
                        trust_remote_code=True
                    )
                    print('–ú–æ–¥–µ–ª—å ICEQ —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU —Å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π')
                except Exception as e:
                    print(f'–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ 4-bit: {e}')
                    try:
                        print('Fallback: –ø–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π...')
                        model = AutoModelForCausalLM.from_pretrained(
                            'droyti/ICEQ', 
                            load_in_8bit=True,
                            torch_dtype=torch.float16,
                            device_map="auto",
                            low_cpu_mem_usage=True,
                            trust_remote_code=True,
                            max_memory={0: "6GB", "cpu": "2GB"}
                        )
                        print('‚úÖ ICEQ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å 8-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π')
                        
                    except Exception as e2:
                        print(f'–û—à–∏–±–∫–∞ 8-bit: {e2}. –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU...')
                        torch.cuda.empty_cache()
                        model = AutoModelForCausalLM.from_pretrained(
                            'droyti/ICEQ', 
                            torch_dtype=torch.float16,
                            device_map="cpu",
                            low_cpu_mem_usage=True,
                            trust_remote_code=True
                        )
                        print('‚ö†Ô∏è ICEQ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU (–±–µ–∑ –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏)')
            else:
                print('CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ CPU...')
                model = AutoModelForCausalLM.from_pretrained(
                    'droyti/ICEQ', 
                    torch_dtype=torch.float16,
                    device_map="cpu",
                    low_cpu_mem_usage=True,
                    trust_remote_code=True
                )
                print('‚ö†Ô∏è ICEQ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ CPU')

            return {
                'tokenizer': tokenizer,
                'model': model
            }
            
        except Exception as e:
            print(f'‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ ICEQ: {e}')
            print('ICEQ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è —Ç–æ–ª—å–∫–æ DeepSeek')
            return None

    def __load_prompt(self, filename: str) -> str:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞
        
        Args:
            filename (str): –Ω–∞–∑–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ —Å –ø—Ä–æ–º–ø—Ç–æ–º
            
        Returns:
            str: —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –ø—Ä–æ–º–ø—Ç–∞
        """
        with open(filename, 'r', encoding='utf8') as f:
            return f.read()

    def __filter_chunks(self, chunk: str, min_words_in_chunk: int) -> bool:
        """
        –§–∏–ª—å—Ç—Ä—É–µ—Ç —á–∞–Ω–∫–∏ —Ç–µ–∫—Å—Ç–∞ –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–ª–æ–≤
        
        Args:
            chunk (str): —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
            min_words_in_chunk (int): –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ–≤
            
        Returns:
            bool: True, –µ—Å–ª–∏ —á–∞–Ω–∫ –ø–æ–¥—Ö–æ–¥–∏—Ç, False - –µ—Å–ª–∏ –Ω–µ—Ç
        """
        result = chunk and len(chunk.split()) > min_words_in_chunk
        return result

    def __get_central_objects(
            self,
            kmeans: KMeans,
            embeddings: np.ndarray,
            objects: np.ndarray
    ) -> np.ndarray:
        
        '''
        –ù–∞—Ö–æ–¥–∏—Ç —Å–∞–º—ã–µ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            kmeans (sklearn.KMeans): –æ–±—É—á–µ–Ω–Ω—ã–π –æ–±—ä–µ–∫—Ç —Ç–∏–ø–∞ sklearn.KMeans
            embeddings (np.ndarray): –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä–µ–∫—Ç–æ–≤
            objects (np.ndarray): —Å–∞–º–∏ –æ–±—ä–µ–∫—Ç—ã

        –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
            central_objects: —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã –∏–∑ objects –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        '''
        
        print('–ü–æ–∏—Å–∫ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...')
        centroids = kmeans.cluster_centers_
        labels = kmeans.labels_
        distances = np.linalg.norm(embeddings - centroids[labels], axis=1)

        central_indices = [
            np.where(labels == i)[0][np.argmin(distances[labels == i])]
            for i in range(kmeans.n_clusters)
        ]

        return objects[central_indices]

    def __get_questions(self, llm: str, text_content: str, questions_num: int) -> list[dict]:

        '''
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –∑–∞–ø—Ä–æ—Å LLM –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            llm (str): LLM –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
            text_content (str): –¢–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            questions_num (int): –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤

        –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (list[dict]): –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        '''

        print(f"üîç === –ù–ê–ß–ê–õ–û –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø –î–õ–Ø –ú–û–î–ï–õ–ò {llm.upper()} ===")
        print(f"üìä –ó–∞–ø—Ä–æ—à–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {questions_num}")
        print(f"üìù –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(text_content)} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"üìÑ –ù–∞—á–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {text_content[:200]}...")

        match llm:
            case 'deepseek':
                print('ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é Deepseek API...')
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        print("‚ö° –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞...")
                        response_text = loop.run_until_complete(
                            generate_questions_deepseek(text_content, questions_num)
                        )
                        print("üì• –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥...")
                    finally:
                        loop.close()
                    
                    print(f"üìã –û—Ç–≤–µ—Ç –æ—Ç DeepSeek API –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(response_text)} —Å–∏–º–≤–æ–ª–æ–≤).")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                    print("üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ parse_questions...")
                    parsed_questions = parse_questions(response_text)
                    print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
                    
                    if not parsed_questions:
                        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: parse_questions –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫!")
                        print(f"üìÑ –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {response_text[:1000]}...")
                    
                    return parsed_questions
                except Exception as e:
                    print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å DeepSeek API: {e}')
                    print(f"üîç –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
                    import traceback
                    print(f"üìã –°—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤: {traceback.format_exc()}")
                    return []
            
            case 'qwen':
                print('ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤ —Å –ø–æ–º–æ—â—å—é Qwen API...')
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ñ—É–Ω–∫—Ü–∏—é
                    loop = asyncio.new_event_loop()
                    asyncio.set_event_loop(loop)
                    try:
                        print("‚ö° –ó–∞–ø—É—Å–∫ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞...")
                        response_text = loop.run_until_complete(
                            generate_questions_qwen(text_content, questions_num)
                        )
                        print("üì• –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∑–∞–≤–µ—Ä—à–µ–Ω, –Ω–∞—á–∏–Ω–∞–µ–º –ø–∞—Ä—Å–∏–Ω–≥...")
                    finally:
                        loop.close()
                    
                    print(f"üìã –û—Ç–≤–µ—Ç –æ—Ç Qwen API –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(response_text)} —Å–∏–º–≤–æ–ª–æ–≤).")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                    print("üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ parse_questions...")
                    parsed_questions = parse_questions(response_text)
                    print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
                    
                    if not parsed_questions:
                        print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: parse_questions –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫!")
                        print(f"üìÑ –°—ã—Ä–æ–π –æ—Ç–≤–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {response_text[:1000]}...")
                    
                    return parsed_questions
                except Exception as e:
                    print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å Qwen API: {e}')
                    print(f"üîç –¢–∏–ø –æ—à–∏–±–∫–∏: {type(e).__name__}")
                    import traceback
                    print(f"üìã –°—Ç–µ–∫ –≤—ã–∑–æ–≤–æ–≤: {traceback.format_exc()}")
                    return []
            
            case 'iceq':
                print('ü§ñ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–Ω–æ–π ICEQ Model...')
                if self.iceq_model is None:
                    raise ValueError("–ú–æ–¥–µ–ª—å ICEQ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
                user_prompt = self.__user_prompt_template \
                    .replace(QUESTIONS_NUM_PROMPT_TAG, str(questions_num)) \
                    .replace(CHUNKS_PROMPT_TAG, text_content)

                prompt = self.__system_prompt + '\n' + user_prompt
                
                print(f"üìÑ –°—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç (–¥–ª–∏–Ω–∞: {len(prompt)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print(f"üìÑ –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤ –ø—Ä–æ–º–ø—Ç–∞: {prompt[:300]}...")
                
                messages = [
                    {'role': 'user', 'content': prompt}
                ]

                # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Chat Template
                print("üîÑ –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ chat template...")
                text = self.iceq_model['tokenizer'].apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True
                )
                
                print(f"üìÑ –§–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ template (–¥–ª–∏–Ω–∞: {len(text)} —Å–∏–º–≤–æ–ª–æ–≤)")
                print(f"üìÑ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {text[:200]}...")
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏
                model_device = next(self.iceq_model['model'].parameters()).device
                print(f"üñ•Ô∏è –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏: {model_device}")
                
                print("üî§ –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è...")
                model_inputs = self.iceq_model['tokenizer']([text], return_tensors='pt').to(model_device)
                print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –∑–∞–ø—Ä–æ—Å–µ: {model_inputs.input_ids.shape[1]}")

                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–æ–ø—Ä–æ—Å–æ–≤
                print("üéØ –ó–∞–ø—É—Å–∫ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏...")
                generated_ids = self.iceq_model['model'].generate(
                    **model_inputs,
                    max_new_tokens=32_000
                )
                generated_ids = [
                    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
                ]

                print("üî§ –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞...")
                response = self.iceq_model['tokenizer'].batch_decode(generated_ids, skip_special_tokens=True)[0]
                print(f'üìã –û—Ç–≤–µ—Ç –æ—Ç ICEQ Model –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(response)} —Å–∏–º–≤–æ–ª–æ–≤).')
                print(f"üìÑ –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ –æ—Ç–≤–µ—Ç–∞: {response[:500]}...")
                
                # –õ–æ–≥–∏—Ä—É–µ–º –ø–µ—Ä–µ–¥ –ø–∞—Ä—Å–∏–Ω–≥–æ–º
                print("üîç –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ parse_questions...")
                parsed_questions = parse_questions(response)
                print(f"‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–æ–ª—É—á–µ–Ω–æ {len(parsed_questions)} –≤–æ–ø—Ä–æ—Å–æ–≤")
                
                if not parsed_questions:
                    print("‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: parse_questions –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫!")
                    print(f"üìÑ –ü–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {response}")

                return parsed_questions
        
        print("=== –ö–û–ù–ï–¶ –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø ===")

    def __generate_iceq(self, text: str, num_questions: int) -> List[Dict]:
        if not self.iceq_model:
            return []
        
        tokenizer = self.iceq_model['tokenizer']
        model = self.iceq_model['model']
        
        # –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –±–æ–ª–µ–µ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤
        prompt = f"""–°–æ–∑–¥–∞–π {num_questions} —Å–ª–æ–∂–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–∞ —Å 4 –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –æ—Ç–≤–µ—Ç–æ–≤ –ø–æ —Ç–µ–∫—Å—Ç—É. 
–ö–∞–∂–¥—ã–π –≤–æ–ø—Ä–æ—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å —Ä–∞–∑–Ω–æ–≥–æ —Ç–∏–ø–∞: –Ω–∞ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ñ–∞–∫—Ç–æ–≤, –Ω–∞ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π, 
–∏ –Ω–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –∑–Ω–∞–Ω–∏–π.

–¢–µ–∫—Å—Ç:
{text[:1500]} 

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
1. –í–æ–ø—Ä–æ—Å: [–≤–æ–ø—Ä–æ—Å]
–í–∞—Ä–∏–∞–Ω—Ç—ã: A) [–≤–∞—Ä–∏–∞–Ω—Ç] B) [–≤–∞—Ä–∏–∞–Ω—Ç] C) [–≤–∞—Ä–∏–∞–Ω—Ç] D) [–≤–∞—Ä–∏–∞–Ω—Ç]
–û—Ç–≤–µ—Ç: [–±—É–∫–≤–∞]

2. –í–æ–ø—Ä–æ—Å: [–≤–æ–ø—Ä–æ—Å]
–í–∞—Ä–∏–∞–Ω—Ç—ã: A) [–≤–∞—Ä–∏–∞–Ω—Ç] B) [–≤–∞—Ä–∏–∞–Ω—Ç] C) [–≤–∞—Ä–∏–∞–Ω—Ç] D) [–≤–∞—Ä–∏–∞–Ω—Ç]
–û—Ç–≤–µ—Ç: [–±—É–∫–≤–∞]"""

        try:
            inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ —Ç–æ –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ, —á—Ç–æ –∏ –º–æ–¥–µ–ª—å
            device = next(model.parameters()).device
            inputs = {k: v.to(device) for k, v in inputs.items()}
            
            # –£–ª—É—á—à–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –¥–ª—è 8-–±–∏—Ç–Ω–æ–≥–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=600,  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–ª—è –±–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    do_sample=True,      # –í–∫–ª—é—á–∞–µ–º —Å–µ–º–ø–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –±–æ–ª–µ–µ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
                    temperature=0.7,     # –£–º–µ—Ä–µ–Ω–Ω–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞
                    top_p=0.9,           # –í—ã—Å–æ–∫–∏–π top_p –¥–ª—è –±–æ–ª—å—à–µ–≥–æ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                    top_k=50,            # –£–º–µ—Ä–µ–Ω–Ω—ã–π top_k
                    num_beams=2,         # –ù–µ–±–æ–ª—å—à–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á–µ–π –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
                    pad_token_id=tokenizer.eos_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    early_stopping=True
                )
            
            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = generated_text[len(prompt):].strip()
            
            return self.__parse_iceq_response(response, num_questions)
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å ICEQ: {e}")
            return []

    def __parse_iceq_response(self, response: str, num_questions: int) -> List[Dict]:
        """–ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ ICEQ –≤ —É–ø—Ä–æ—â–µ–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        questions = []
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤–æ–ø—Ä–æ—Å–æ–≤
        question_pattern = r'(\d+)\.\s*–í–æ–ø—Ä–æ—Å:\s*(.+?)(?=\d+\.\s*–í–æ–ø—Ä–æ—Å:|$)'
        matches = re.findall(question_pattern, response, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            question_num, question_block = match
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞
            lines = question_block.strip().split('\n')
            question_text = lines[0].strip()
            
            # –ò—â–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤
            answers = []
            correct_answer = None
            
            for line in lines[1:]:
                line = line.strip()
                
                # –í–∞—Ä–∏–∞–Ω—Ç—ã A), B), C), D)
                variant_match = re.match(r'([ABCD])\)\s*(.+)', line)
                if variant_match:
                    letter, text = variant_match.groups()
                    answers.append({
                        'answer': text.strip(),
                        'is_correct': False,
                        'letter': letter
                    })
                
                # –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
                answer_match = re.match(r'–û—Ç–≤–µ—Ç:\s*([ABCD])', line)
                if answer_match:
                    correct_answer = answer_match.group(1)
            
            # –û—Ç–º–µ—á–∞–µ–º –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
            if correct_answer and answers:
                for answer in answers:
                    if answer['letter'] == correct_answer:
                        answer['is_correct'] = True
                        break
                
                # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω–æ–µ –ø–æ–ª–µ letter
                for answer in answers:
                    del answer['letter']
                
                questions.append({
                    'question': question_text,
                    'answers': answers
                })
            
            if len(questions) >= num_questions:
                break
        
        return questions

    def generate(
            self, 
            text: str, 
            questions_num: int,
            llm: Literal['deepseek', 'qwen', 'iceq'] = 'iceq'
    ) -> list[dict]:

        '''
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –ø–æ —Ç–µ–∫—Å—Ç—É

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text (str): —Ç–µ–∫—Å—Ç, –ø–æ –∫–æ—Ç–æ—Ä–æ–º—É –Ω–∞–¥–æ –∑–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å—ã
            questions_num (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤
            llm (Literal['deepseek', 'qwen', 'iceq']), optional:
                —è–∑—ã–∫–æ–≤–∞—è –º–æ–¥–µ–ª—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
                    - deepseek: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ DeepSeek API
                    - qwen: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ Qwen API
                    - iceq: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏

        –í–æ–∑–≤—Ä–∞—â–∞–µ–º–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ:
            questions (list[dict]): —Å–ø–∏—Å–æ–∫ –≤–æ–ø—Ä–æ—Å–æ–≤
        '''

        if llm == 'deepseek' and self.deepseek_client is None:
            self.deepseek_client = self.__init_deepseek()
        
        if llm == 'iceq' and self.iceq_model is None:
            self.iceq_model = self.__init_iceq()
            if self.iceq_model is None:
                raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å ICEQ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 'deepseek' –∏–ª–∏ 'qwen' –≤–º–µ—Å—Ç–æ 'iceq'.")

        print(f'–ù–∞—á–∞–ª–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {questions_num} –≤–æ–ø—Ä–æ—Å–æ–≤...')
        
        # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: —Ä–∞–∑–¥–µ–ª—è–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –∏ —Å–º–æ—Ç—Ä–∏–º, —Ö–≤–∞—Ç–∏—Ç –ª–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤
        initial_chunks = text.split('\n\n')  # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ –¥–≤–æ–π–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º (–ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã)
        initial_chunks = [chunk.strip() for chunk in initial_chunks if chunk.strip()]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–º—ã—Å–ª–æ–≤—ã—Ö –±–ª–æ–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
        if len(initial_chunks) < questions_num:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ –æ–¥–∏–Ω–∞—Ä–Ω—ã–º –ø–µ—Ä–µ–Ω–æ—Å–∞–º
            backup_chunks = text.split('\n')
            backup_chunks = [chunk.strip() for chunk in backup_chunks if chunk.strip() and len(chunk.split()) > 5]
            
            if len(backup_chunks) < questions_num:
                raise ValueError(
                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã—Ö –±–ª–æ–∫–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {questions_num} –≤–æ–ø—Ä–æ—Å–æ–≤. "
                    f"–ù–∞–π–¥–µ–Ω–æ {len(backup_chunks)} –±–ª–æ–∫–æ–≤. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –¥–æ {len(backup_chunks)} "
                    f"–∏–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å –±–æ–ª—å—à–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."
                )
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–∞–π–º–µ—Ä
        import time
        start_time = time.time()
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ü–µ–Ω–∫—É –≤—Ä–µ–º–µ–Ω–∏
        time_estimate = self.estimate_generation_time(text, questions_num, llm)

        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        chunks = text.split('\n')
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —á–∞–Ω–∫–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        mean_chunk_len = np.array([len(chunk) for chunk in chunks]).mean()
        min_words_in_chunk = mean_chunk_len * 0.15

        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
        chunks = list(filter(lambda chunk: self.__filter_chunks(chunk, min_words_in_chunk), chunks))
        chunks = np.array(chunks)
        print(f'–ü–æ–ª—É—á–µ–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏.')

        # –ï—Å–ª–∏ —á–∞–Ω–∫–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        if len(chunks) < MIN_CLUSTERS_NUM:
            print(f'–ß–∞–Ω–∫–æ–≤ —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ ({len(chunks)}), –∏—Å–ø–æ–ª—å–∑—É–µ–º —É–ø—Ä–æ—â–µ–Ω–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...')
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –¥–ª—è ICEQ
            if llm == 'iceq':
                questions = self.__generate_iceq(text, questions_num)
            else:
                # –î–ª—è –¥—Ä—É–≥–∏—Ö LLM –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã
                prompt = self.__user_prompt_template \
                    .replace(QUESTIONS_NUM_PROMPT_TAG, str(questions_num)) \
                    .replace(CHUNKS_PROMPT_TAG, text[:2000])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
                return self.__get_questions(llm, prompt, questions_num)

        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ —á–∞–Ω–∫–æ–≤
        print('–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...')
        clustering_embeddings = self.__clustering_model.encode(
            chunks,
            convert_to_tensor=True,
            normalize_embeddings=True,
            device=self.device
        ).cpu().numpy()
        print('–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –≤—ã—á–∏—Å–ª–µ–Ω—ã.')

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        paragraph_num = len(text.split('\n'))
        clusters_num = min(
            MAX_CLUSTERS_NUM,
            max(MIN_CLUSTERS_NUM, int(paragraph_num * DATA_PART))
        )
        print(f'–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {clusters_num}')

        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        print('–ó–∞–ø—É—Å–∫ K-means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...')
        kmeans = KMeans(n_clusters=clusters_num, random_state=42)
        kmeans.fit(clustering_embeddings)
        print('–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.')

        # –ü–æ–∏—Å–∫ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –≤ –∫–∞–∂–¥–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ
        target_chunks = self.__get_central_objects(kmeans, clustering_embeddings, chunks)
        print('–ü–æ–∏—Å–∫ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...')
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(target_chunks)} —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã—Ö —á–∞–Ω–∫–æ–≤")
        
        # –õ–æ–≥–∏—Ä—É–µ–º –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏
        for i, chunk in enumerate(target_chunks):
            print(f"üîç –ß–∞–Ω–∫ {i+1}: {chunk[:100]}...")
        
        print('–ü–µ—Ä–µ–¥–∞—á–∞ —á–∞–Ω–∫–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...')
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –æ—Ç–æ–±—Ä–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        text_for_generation = '\n\n'.join(target_chunks)
        print(f"üìù –ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ (–¥–ª–∏–Ω–∞: {len(text_for_generation)} —Å–∏–º–≤–æ–ª–æ–≤)")
        print(f"üìÑ –ü–µ—Ä–≤—ã–µ 500 —Å–∏–º–≤–æ–ª–æ–≤ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {text_for_generation[:500]}...")
        
        print("üéØ –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –¢–û–ß–ö–ê: –í—ã–∑—ã–≤–∞–µ–º __get_questions")
        questions = self.__get_questions(llm, text_for_generation, questions_num)
        print(f"üìã –†–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç __get_questions: {len(questions) if questions else 0} –≤–æ–ø—Ä–æ—Å–æ–≤")

        # –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å—ã –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫
        if not questions:
            print('‚ùå –ö–†–ò–¢–ò–ß–ù–û: –í–æ–ø—Ä–æ—Å—ã –Ω–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã!')
            print("üîç –ü–æ–ø—ã—Ç–∫–∞ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º—ã:")
            print(f"  - –ú–æ–¥–µ–ª—å: {llm}")
            print(f"  - –î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(text_for_generation)}")
            print(f"  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞–Ω–∫–æ–≤: {len(target_chunks)}")
            print(f"  - –ó–∞–ø—Ä–æ—à–µ–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {questions_num}")
            return []
        else:
            print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –≤–æ–ø—Ä–æ—Å—ã: {len(questions)}")
            # –ö—Ä–∞—Ç–∫–æ –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏
            for i, q in enumerate(questions[:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                print(f"  {i+1}. {q.get('question', 'NO_QUESTION')[:50]}...")

        try:
            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π —á–µ—Ä–µ–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
            print('–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞...')
            doc_embeddings = self.__search_model.encode(
                target_chunks,
                prompt_name='search_document',
                device=self.device
            )
            query_embeddings = self.__search_model.encode(
                [q['question'] for q in questions],
                prompt_name='search_query',
                device=self.device
            )
            print('–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω—ã.')

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            if query_embeddings is None or len(query_embeddings.shape) < 2 or query_embeddings.shape[0] == 0:
                print("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤. –û–±—ä—è—Å–Ω–µ–Ω–∏—è –±—É–¥—É—Ç –ø—Ä–æ–ø—É—â–µ–Ω—ã.")
                raise ValueError("–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞")

            # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö —á–∞–Ω–∫–æ–≤
            print('–ù–∞—Å—Ç—Ä–æ–π–∫–∞ FAISS –∏–Ω–¥–µ–∫—Å–∞...')
            index = faiss.IndexFlatIP(doc_embeddings.shape[1])  # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ
            index.add(doc_embeddings)
            print('FAISS –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤.')

            # –ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞
            print('–ü–æ–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ —á–∞–Ω–∫–æ–≤...')
            _, indices = index.search(query_embeddings, 1)

            # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∫ –≤–æ–ø—Ä–æ—Å–∞–º
            for question, explanation_chunk in zip(questions, target_chunks[indices]):
                # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏–ª–∞ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π —á–∞–Ω–∫
                if not question.get('explanation'):
                    question['explanation'] = str(explanation_chunk[0])
                
        except Exception as e:
            print(f'‚ö†Ô∏è –û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: {e}')
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –ø—Ä–∏ –æ—à–∏–±–∫–µ
            for q in questions:
                if not q.get('explanation'):
                    q['explanation'] = '–û–±—ä—è—Å–Ω–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑-–∑–∞ –æ—à–∏–±–∫–∏.'

        # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        actual_time = time.time() - start_time
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
        model_names = {
            'deepseek': 'DeepSeek-V3',
            'qwen': 'Qwen-3-235B',
            'iceq': 'ICEQ (–ª–æ–∫–∞–ª—å–Ω–∞—è)'
        }
        model_display = model_names.get(llm, llm.upper())
        
        print()
        print(f'üéâ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!')
        print(f'   ü§ñ –ú–æ–¥–µ–ª—å: {model_display}')
        print(f'   ‚è±Ô∏è  –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –≤—Ä–µ–º—è: {actual_time:.1f} —Å–µ–∫ ({actual_time/60:.1f} –º–∏–Ω)')
        print(f'   üìà –û–∂–∏–¥–∞–ª–æ—Å—å: {time_estimate["estimated_seconds"]} —Å–µ–∫')
        print(f'   üìä –†–∞–∑–Ω–∏—Ü–∞: {actual_time - time_estimate["estimated_seconds"]:.1f} —Å–µ–∫')
        print(f'   üìù –†–µ–∑—É–ª—å—Ç–∞—Ç: {len(questions)} –≤–æ–ø—Ä–æ—Å–æ–≤')
        print()

        # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–¢–õ–ê–î–ö–ê: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –ø–µ—Ä–µ–¥ –≤–æ–∑–≤—Ä–∞—Ç–æ–º
        print(f"üîç === –§–ò–ù–ê–õ–¨–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –°–¢–†–£–ö–¢–£–†–´ –î–ê–ù–ù–´–• ===")
        print(f"üìä –¢–∏–ø –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {type(questions)}")
        print(f"üìä –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤: {len(questions) if questions else 0}")
        
        if questions:
            print(f"üîç –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–µ—Ä–≤–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞:")
            first_q = questions[0]
            print(f"  - –¢–∏–ø: {type(first_q)}")
            print(f"  - –ö–ª—é—á–∏: {list(first_q.keys()) if isinstance(first_q, dict) else 'NOT_DICT'}")
            if isinstance(first_q, dict):
                print(f"  - question: {first_q.get('question', 'MISSING')[:50]}...")
                print(f"  - answers: {len(first_q.get('answers', []))} –æ—Ç–≤–µ—Ç–æ–≤")
                print(f"  - explanation: {'–¥–∞' if first_q.get('explanation') else '–Ω–µ—Ç'}")
                
                if first_q.get('answers'):
                    first_answer = first_q['answers'][0]
                    print(f"  - –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø–µ—Ä–≤–æ–≥–æ –æ—Ç–≤–µ—Ç–∞: {first_answer}")
        else:
            print("‚ùå –ö–†–ò–¢–ò–ß–ù–û: –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫!")
        
        print("=== –ö–û–ù–ï–¶ –§–ò–ù–ê–õ–¨–ù–û–ô –ü–†–û–í–ï–†–ö–ò ===")
        
        return questions

    def estimate_generation_time(self, text: str, questions_num: int, llm: str = 'iceq') -> dict:
        """
        –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤–æ–ø—Ä–æ—Å–æ–≤
        
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
            text (str): —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            questions_num (int): –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤
            llm (str): –∏—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –º–æ–¥–µ–ª—å
            
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
            dict: –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤—Ä–µ–º–µ–Ω–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
        """
        text_length = len(text)
        word_count = len(text.split())
        
        if llm == 'iceq':
            # –ë–∞–∑–æ–≤–æ–µ –≤—Ä–µ–º—è –¥–ª—è ICEQ –º–æ–¥–µ–ª–∏ —Å 8-–±–∏—Ç–Ω—ã–º –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ–º
            base_time_per_question = 3.5  # —Å–µ–∫—É–Ω–¥ –Ω–∞ –≤–æ–ø—Ä–æ—Å
            text_factor = min(text_length / 1000, 3.0)  # –¥–æ 3x –∑–∞ –¥–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
            complexity_factor = 1 + (questions_num * 0.1)  # —É—Å–ª–æ–∂–Ω–µ–Ω–∏–µ —Å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º
            
            estimated_time = (base_time_per_question * questions_num * text_factor * complexity_factor)
        else:
            # –î–ª—è DeepSeek API
            base_time_per_question = 2.0  # –±—ã—Å—Ç—Ä–µ–µ —á–µ—Ä–µ–∑ API
            text_factor = min(text_length / 1500, 2.0)
            estimated_time = base_time_per_question * questions_num * text_factor
        
        return {
            'estimated_seconds': int(estimated_time),
            'estimated_minutes': round(estimated_time / 60, 1),
            'text_length': text_length,
            'word_count': word_count,
            'questions_count': questions_num,
            'model': llm
        }


if __name__ == '__main__':
    print('–ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —Å—Ü–µ–Ω–∞—Ä–∏—è...')
    with open('test_data/markdown.md', 'r', encoding='utf8') as f:
        text = f.read()

    generator = QuestionsGenerator()
    questions = generator.generate(text, 10)

    print('–†–µ–∑—É–ª—å—Ç–∞—Ç:')
    for i, q in enumerate(questions, 1):
        print(f'\n–í–æ–ø—Ä–æ—Å {i}: {q["question"]}')
        print('–í–∞—Ä–∏–∞–Ω—Ç—ã –æ—Ç–≤–µ—Ç–æ–≤:')
        for ans in q['answers']:
            mark = '[+]' if ans["is_correct"] else '[ ]'
            print(f'  {mark} {ans["answer"]}')
        print(f'–û–±—ä—è—Å–Ω–µ–Ω–∏–µ: {q["explanation"]}')
